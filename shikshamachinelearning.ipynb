{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ba7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.what is parameter\n",
    "#Parameter: In machine learning and statistics,\n",
    "#a parameter is a numerical value that helps define a model's behavior.\n",
    "#For example, in a linear regression model, the coefficients (like slope and intercept) are parameters\n",
    "#that determine how the input variables influence the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3135805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is Corelation?\n",
    "#Correlation measures the relationship between two variables. \n",
    "#If two variables are positively correlated, an increase in one tends to lead to an increase in the other.\n",
    "#If they are negatively correlated, an increase in one tends to lead to a decrease in the other.\n",
    "#Correlation doesn't necessarily imply causation—it just shows a connection between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daeae2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what does negative corealation means?\n",
    "#When two variables have a negative correlation, it means that as one increases, the other decreases.\n",
    "#For example, in a business setting, if the price of a product increases,\n",
    "#sales may decrease—that’s a negative correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db990ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.define machine learning what are the main components of machine learning?\n",
    "#the subfield of computer science  that gives computers the ablity to learn without\n",
    "#explicitly programmed.\n",
    "#Machine learn patterns from the data and replicate the same in future.\n",
    "#Main components of machine learning are-\n",
    "#Features – Characteristics or attributes used to train the model, such as age, price, or temperature.\n",
    "\n",
    "###Model – The mathematical structure that learns patterns from data and makes predictions.\n",
    "\n",
    "#Training – The process where the model learns patterns from input data using algorithms.\n",
    "\n",
    "#Testing & Validation – Checking the model’s performance on unseen data to ensure accuracy.\n",
    "\n",
    "#Optimization & Evaluation – Techniques like hyperparameter tuning to refine model performance.\n",
    "\n",
    "#Loss Function – Measures how far the model’s predictions are from actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddbe19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does loss value help in determining whether the model is good or not?\n",
    "#Loss value helps determine whether the model is performing well. \n",
    "#It quantifies the difference between actual values and predicted values:\n",
    "\n",
    "#Low loss indicates the model makes accurate predictions.\n",
    "\n",
    "#High loss suggests the model needs improvement (e.g., more data, better features, or different algorithms).\n",
    "\n",
    "#Optimization techniques like gradient descent adjust model parameters to minimize loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b35a0696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are continuous and categorical variables?\n",
    "#Continuous Variables: These are numerical values that can take any number within a given range.\n",
    "#They have a measurable quantity and can have decimals. \n",
    "#Examples include height, temperature, or stock prices.\n",
    "#Categorical Variables: These represent distinct groups or categories. \n",
    "#They can be either nominal (no inherent order, like colors or city names)\n",
    "#or ordinal (ordered categories, like star ratings or education level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5d4534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How do we handle categorical variables in Machine Learning? What are the common t echniques?\n",
    "#Since machine learning models work with numbers, categorical variables need to be transformed into a numerical format.\n",
    "#Common techniques include:\n",
    "\n",
    "#Label Encoding: Assigns a unique integer to each category.\n",
    "#Suitable for ordinal data (e.g., \"Low,\" \"Medium,\" \"High\" → 0, 1, 2).\n",
    "\n",
    "#One-Hot Encoding: Converts categories into binary vectors \n",
    "#(e.g., \"Red,\" \"Blue,\" \"Green\" → [1,0,0], [0,1,0], [0,0,1]).\n",
    "#Best for nominal categorical data.\n",
    "\n",
    "#Target guided oridnal encoding\n",
    "#based on relationship with target variable\n",
    "#useful when we have large number of unique categories in categorical data.\n",
    "#categorial groups with mean and median of corresponding target variable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae409618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What do you mean by training and testing a dataset\n",
    "#Training a Dataset: This is the process where a machine learning model learns from labeled data. \n",
    "#The model adjusts its parameters by analyzing patterns in the training dataset \n",
    "#so it can make accurate predictions.\n",
    "\n",
    "#Testing a Dataset: After training, the model is tested on unseen data to evaluate its performance.\n",
    "#The testing dataset helps determine if the model generalizes well to new information ,\n",
    "#or if it has overfitted to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6b3a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is sklearn.preprocessing?\n",
    "#sklearn.preprocessing is a module in Scikit-Learn, a popular Python library for machine learning.\n",
    "#It provides various functions to scale, normalize, encode, and transform data before feeding it into a machine learning model.\n",
    "#Some common preprocessing techniques include:\n",
    "#Standardization (e.g., StandardScaler) – Adjusts features to have zero mean and unit variance.\n",
    "\n",
    "#Normalization (e.g., MinMaxScaler) – Scales features to a specific range (e.g., 0 to 1).\n",
    "\n",
    "#Encoding categorical variables (e.g., OneHotEncoder, LabelEncoder).\n",
    "\n",
    "#Feature transformation (e.g., PolynomialFeatures, PowerTransformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41c0dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is a Test set\n",
    "#A test set is a portion of the dataset that is not used for training but instead helps evaluate the model’s performance. \n",
    "#It ensures that the model generalizes well to unseen data. Typically, datasets are split into:\n",
    "\n",
    "##Training set (e.g., 80%) – Used to train the model.\n",
    "\n",
    "#Test set (e.g., 20%) – Used to assess accuracy and prevent overfitting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65702dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: (4, 2)\n",
      "Testing Set Size: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# How do we split data for model fitting (training and testing) in Python?\n",
    "# How do you approach a Machine Learning problem?\n",
    "#In machine learning, we split data into training and testing sets to evaluate model performance. \n",
    "#The most common way to do this in Python is using train_test_split() from Scikit-Learn:\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.DataFrame({'Feature1': [1, 2, 3, 4, 5], 'Feature2': [10, 20, 30, 40, 50], 'Target': [0, 1, 0, 1, 0]})\n",
    "\n",
    "\n",
    "X = data[['Feature1', 'Feature2']]  # Features\n",
    "y = data['Target']  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Set Size:\", X_train.shape)\n",
    "print(\"Testing Set Size:\", X_test.shape)\n",
    "#A structured approach helps ensure accuracy and efficiency in solving ML problems. \n",
    "#Here’s a step-by-step breakdown:\n",
    "\n",
    "#Understand the Problem – Define the objective, whether it's classification, regression, clustering, etc.\n",
    "\n",
    "#Collect & Explore Data – Perform Exploratory Data Analysis (EDA) to understand patterns, missing values, and distributions.\n",
    "\n",
    "#Preprocess Data – Handle missing values, normalize/scale features, and encode categorical variables.\n",
    "\n",
    "#Feature Engineering – Select or create meaningful features to improve model performance.\n",
    "\n",
    "#Choose a Model – Select an appropriate algorithm (e.g., Decision Trees, Neural network.\n",
    "#Train the Model – Fit the model using training data and optimize hyperparameters.\n",
    "\n",
    "#Evaluate Performance – Use metrics like accuracy, precision, recall, RMSE, etc., to assess the model.\n",
    "\n",
    "#Fine-Tune & Optimize – Adjust hyperparameters, try different models, or improve feature selection.\n",
    "\n",
    "#Deploy & Monitor – Deploy the model and continuously monitor its performance in real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why do we have to perform EDA before fitting a model to the data?\n",
    " #EDA is a crucial step in machine learning because it helps us understand the dataset before applying any model. \n",
    "    #Here’s why it’s important:\n",
    "\n",
    "#Detects Missing Values & Outliers – Identifies anomalies that could negatively impact model performance.\n",
    "\n",
    "#Feature Selection – Helps determine which variables are most relevant for prediction.\n",
    "\n",
    "#Data Distribution Insights – Allows us to visualize how data is spread, ensuring appropriate transformations.\n",
    "\n",
    "#Improves Model Accuracy – Cleaning and preprocessing data before training leads to better results.\n",
    "\n",
    "#Prevents Overfitting – Helps avoid models learning noise instead of meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bdfac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is Correlation?\n",
    "#Correlation measures the relationship between two variables. \n",
    "#It tells us whether changes in one variable are associated with changes in another.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "701a3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what does negative corealation means?\n",
    "#When two variables have a negative correlation, it means that as one increases, the other decreases.\n",
    "#For example, in a business setting, if the price of a product increases,\n",
    "#sales may decrease—that’s a negative correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dd74364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Coefficient: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "#How can you find correlation between variables in Python?\n",
    "#In Python, you can calculate correlation using NumPy or Pandas.\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "correlation_matrix = np.corrcoef(x, y)\n",
    "print(\"Correlation Coefficient:\", correlation_matrix[0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4926994b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "A  1.0  1.0\n",
      "B  1.0  1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [2, 4, 6, 8, 10]})\n",
    "\n",
    "# Compute correlation\n",
    "print(data.corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e1443fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is causation? Explain difference between correlation and causation with an example\n",
    "#Causation means that one event directly causes another.\n",
    "#If variable X changes and directly leads to a change in variable Y, then X causes Y.\n",
    "\n",
    "#Difference Between Correlation and Causation\n",
    "#Correlation shows a relationship between two variables but does not imply causation.\n",
    "\n",
    "#Causation means one variable directly influences another.\n",
    "\n",
    "#Example\n",
    "#Correlation: Ice cream sales and drowning incidents are positively correlated. \n",
    "#However, eating ice cream does not cause drowning.\n",
    "#Instead, hot weather is a third factor influencing both.\n",
    "\n",
    "#Causation: Smoking causes lung cancer because harmful chemicals damage lung cells.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5678597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is an Optimizer? What are different types of optimizers?\n",
    "#An optimizer is an algorithm used in machine learning and deep learning \n",
    "#to adjust model parameters (weights) to minimize the loss function.\n",
    "#It helps the model learn by improving predictions over time.\n",
    "#Gradient Descent\n",
    "#Stochastic Gradient Descent (SGD)\n",
    "#Adam (Adaptive Moment Estimation)\n",
    "#RMSprop (Root Mean Square Propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a90c235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is sklearn.linear_model?\n",
    "#sklearn.linear_model is a module in Scikit-Learn that\n",
    "#provides various linear models for regression and classification.\n",
    "#Some key models include:\n",
    "\n",
    "#Linear Regression (LinearRegression) – Fits a linear model using ordinary least squares.\n",
    "\n",
    "##Ridge Regression (Ridge) – Adds L2 regularization to prevent overfitting.\n",
    "\n",
    "#Lasso Regression (Lasso) – Uses L1 regularization for feature selection.\n",
    "\n",
    "#Logistic Regression (LogisticRegression) – Used for binary classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1130db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What does model.fit() do? What arguments must be given?\n",
    "#model.fit() is used to train a machine learning model\n",
    "#by adjusting its parameters based on the given training data.\n",
    "#It learns patterns from the input features (X_train) and their corresponding labels (y_train).\n",
    "\n",
    "#Common Arguments for model.fit()\n",
    "#X_train – The input features (training data).\n",
    "\n",
    "#y_train – The target labels (for supervised learning).\n",
    "\n",
    "#batch_size – Number of samples per gradient update (for deep learning models).\n",
    "\n",
    "#epochs – Number of times the model iterates over the dataset.\n",
    "\n",
    "#verbose – Controls the amount of output displayed during training\n",
    "#(0 = silent, 1 = progress bar, 2 = one line per epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a822954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What does model.predict() do? What arguments must be given?\n",
    "#model.fit() is used to train a machine learning model\n",
    "#by adjusting its parameters based on the given training data.\n",
    "#It learns patterns from the input features (X_train) and their corresponding labels (y_train).\n",
    "\n",
    "#Common Arguments for model.fit()\n",
    "#X_train – The input features (training data).\n",
    "\n",
    "#y_train – The target labels (for supervised learning).\n",
    "\n",
    "#batch_size – Number of samples per gradient update (for deep learning models).\n",
    "\n",
    "#epochs – Number of times the model iterates over the dataset.\n",
    "\n",
    "#verbose – Controls the amount of output displayed during training \n",
    "#(0 = silent, 1 = progress bar, 2 = one line per epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24d7853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20.#What are continuous and categorical variables?\n",
    "#Continuous Variables: These are numerical values that can take any number within a given range.\n",
    "#They have a measurable quantity and can have decimals. \n",
    "#Examples include height, temperature, or stock prices.\n",
    "#Categorical Variables: These represent distinct groups or categories. \n",
    "#They can be either nominal (no inherent order, like colors or city names)\n",
    "#or ordinal (ordered categories, like star ratings or education level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f58e7330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#21. What is feature scaling? How does it help in Machine Learning?\n",
    " #Feature scaling is a preprocessing technique used in machine learning\n",
    "    #to transform numerical features into a similar scale. \n",
    "    #It ensures that all features contribute equally to the model\n",
    "    # preventing bias from features with larger values\n",
    "    #Why is Feature Scaling Important?\n",
    "#Improves Model Performance – Some algorithms (like gradient descent-based models)\n",
    "#require features to be on the same scale for efficient learning.\n",
    "\n",
    "#Prevents Bias – Features with larger values can dominate the learning process if not scaled properly.\n",
    "\n",
    "#Enhances Convergence – Scaling helps optimization algorithms converge faster.\n",
    "\n",
    "#Essential for Distance-Based Models – Algorithms like KNN and SVM rely on feature distances,\n",
    "#making scaling crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c81c550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n",
      "[[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n",
      "[[-1. -1.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "#22.How do we perform scaling in Python?\n",
    "#Python provides several methods for feature scaling using Scikit-Learn3:\n",
    "\n",
    "#1. Standardization (Z-score normalization)\n",
    "#Transforms data to have zero mean and unit variance.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[100, 200], [300, 400], [500, 600]])\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)\n",
    "#2. Min-Max Scaling\n",
    "#Scales values between 0 and 1.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)\n",
    "#3. Robust Scaling\n",
    "#Handles outliers by scaling based on median and interquartile range.\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f88c2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#23.#What is sklearn.preprocessing?\n",
    "#sklearn.preprocessing is a module in Scikit-Learn, a popular Python library for machine learning.\n",
    "#It provides various functions to scale, normalize, encode, and transform data before feeding it into a machine learning model.\n",
    "#Some common preprocessing techniques include:\n",
    "#Standardization (e.g., StandardScaler) – Adjusts features to have zero mean and unit variance.\n",
    "\n",
    "#Normalization (e.g., MinMaxScaler) – Scales features to a specific range (e.g., 0 to 1).\n",
    "\n",
    "#Encoding categorical variables (e.g., OneHotEncoder, LabelEncoder).\n",
    "\n",
    "#Feature transformation (e.g., PolynomialFeatures, PowerTransformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5f9abe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: (4, 2)\n",
      "Testing Set Size: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# 24.How do we split data for model fitting (training and testing) in Python?\n",
    "##In machine learning, we split data into training and testing sets to evaluate model performance. \n",
    "#The most common way to do this in Python is using train_test_split() from Scikit-Learn:\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.DataFrame({'Feature1': [1, 2, 3, 4, 5], 'Feature2': [10, 20, 30, 40, 50], 'Target': [0, 1, 0, 1, 0]})\n",
    "\n",
    "\n",
    "X = data[['Feature1', 'Feature2']]  # Features\n",
    "y = data['Target']  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Set Size:\", X_train.shape)\n",
    "print(\"Testing Set Size:\", X_test.shape)\n",
    "#A structured approach helps ensure accuracy and efficiency in solving ML problems. \n",
    "#Here’s a step-by-step breakdown:\n",
    "\n",
    "#Understand the Problem – Define the objective, whether it's classification, regression, clustering, etc.\n",
    "\n",
    "#Collect & Explore Data – Perform Exploratory Data Analysis (EDA) to understand patterns, missing values, and distributions.\n",
    "\n",
    "#Preprocess Data – Handle missing values, normalize/scale features, and encode categorical variables.\n",
    "\n",
    "#Feature Engineering – Select or create meaningful features to improve model performance.\n",
    "\n",
    "#Choose a Model – Select an appropriate algorithm (e.g., Decision Trees, Neural network.\n",
    "#Train the Model – Fit the model using training data and optimize hyperparameters.\n",
    "\n",
    "#Evaluate Performance – Use metrics like accuracy, precision, recall, RMSE, etc., to assess the model.\n",
    "\n",
    "#Fine-Tune & Optimize – Adjust hyperparameters, try different models, or improve feature selection.\n",
    "\n",
    "#Deploy & Monitor – Deploy the model and continuously monitor its performance in real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e9dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#25.Explain data encoding\n",
    "#Converting the string into numerical data.\n",
    "#Data encoding is the process of converting information into a specific format for efficient storage,\n",
    "#transmission, or processing.\n",
    "#It ensures that data can be understood by computers and transmitted securely across networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
